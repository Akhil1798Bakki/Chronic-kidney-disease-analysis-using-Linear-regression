---
title: "Chronic Kidney disease dataset"
author: "Bakki Akhil"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Here we are analysing the Chronic kidney disease dataset
# The problem statement in this dataset is
## Estimating the relationship of blood pressure on other independent variables like age, blood Urea, Potassium, sodium, serum creatinine, hemoglobin, blood glucose random finding their effect in changing the blood pressure
# Here we are performing Multiple linear regression on the dataset
# The dependent variable for this dataset is blood pressure and the independent variables are age, blood Urea, Potassium, sodium, serum creatinine, hemoglobin, blood glucose random

# Below we are uploading the dataset
```{r}
data1 = read.csv("D:\\Akhil MBA\\MBA 4 Trimester\\Machine Learning Programming Using R\\CIA 1\\kidney_disease.csv", stringsAsFactors = TRUE)
View(data1)
str(data1)
summary(data1)


```

# Checking the presence of null values in whole dataset
# The below code says there are 470 null values
```{r}
sum(is.na(data1))

```

# Before analysing the data we are checking the presence of missing values in the varaibles
# Finding the null values in each dataset
```{r}
sapply(data1, function(x) sum(is.na(x)))

```

# Replacing thes null values with their means for better operation

```{r}
data1$age[which(is.na(data1$age))] = mean(data1$age, na.rm = TRUE)
data1$bp[which(is.na(data1$bp))] = mean(data1$bp, na.rm = TRUE)
data1$bu[which(is.na(data1$bu))] = mean(data1$bu, na.rm = TRUE)
data1$sc[which(is.na(data1$sc))] = mean(data1$sc, na.rm = TRUE)
data1$su[which(is.na(data1$su))] = mean(data1$su, na.rm = TRUE)
data1$hemo[which(is.na(data1$hemo))] = mean(data1$hemo, na.rm = TRUE)
data1$sg[which(is.na(data1$sg))] = mean(data1$sg, na.rm = TRUE)
data1$al[which(is.na(data1$al))] = mean(data1$al, na.rm = TRUE)
data1$bgr[which(is.na(data1$bgr))] = mean(data1$bgr, na.rm = TRUE)
data1$sod[which(is.na(data1$sod))] = mean(data1$sod, na.rm = TRUE)
data1$pot[which(is.na(data1$pot))] = mean(data1$pot, na.rm = TRUE)

```

# Checking the presence of null values graphically
```{r}
library(Amelia)
missmap(data1, main = "Missing Values vs. Observed")

```

# Problem statement - Building a Multiple linear regression model on dependent variable blood pressure and independent variable age, blood Urea, Potassium, sodium, serum creatinine, hemoglobin, blood glucose random and esstimating their relationship

# Making a subset from the present main dataset
# Here we are selecting the variables according to the problem statement

```{r}
data2 = data1[,c("id", "age",  "bu", "pot", "sod", "sc", "hemo", "bgr", "bp")]
View(data2)
str(data2)

```

# Checking the null values in the new dataset created
```{r}
sum(is.na(data2))
```

# Creating a correlation matrix for finding the relationship between all the data2 variables 
```{r}
data2corr = cor(data2)
data2corr

library(corrplot)
# Plotting the correlation in graphical format
corrplot(data2corr)

```

# Analysing the variables of the model in the dataset
```{r}
library(ggplot2)

#Analysing age variable
ggplot(data2, aes(data2$age))+geom_histogram()

#Analysing Blood Urea variable
ggplot(data2, aes(data2$bu))+geom_histogram()

# Analysing Potassium variable
ggplot(data2, aes(data2$pot))+geom_histogram()

# Analysing Sodium variable
ggplot(data2, aes(data2$sod))+geom_histogram()

# Analysing Serum Creatinine variable
ggplot(data2, aes(data2$sc))+geom_histogram()

#Analysing Hemoglobin variable
ggplot(data2, aes(data2$hemo))+geom_histogram(binwidth = 1)

# Analysing Blood glucose random variable
ggplot(data2, aes(data2$bgr))+geom_histogram()

#Analysing Blood pressure variable
ggplot(data2, aes(data2$bp))+geom_histogram(binwidth = 9)



```

# Performing Multiple linear regression on the problem statement by considering Blood pressure as dependent variable and age, blood Urea, Potassium, sodium, serum creatinine, hemoglobin, blood glucose random as independent variables 

# y = bo + b1x1 + b2x2 + b3x3 + b4x4 + b5x5 + b6x6 + b7x7
# Null Hypothesis = H0 = b0 = b1 = b2 = b3 = b4 = b5 = b6 = b7 = 0
# Alternate Hypothesis = H1 = Atleast one is not equal to zero

# Splitting the data using caTools library

```{r}
library(caTools)
set.seed(100)
split1 = sample.split(data2$id, SplitRatio = 0.7)
split1
summary(split1)

```

#Subsetting the dataset
```{r}
datatrain = subset(data2, split1 == TRUE)
datatest = subset(data2, split1 == FALSE)

dim(datatrain)
dim(datatest)

```

#Developing a linear regression model for the above dependent variable blood pressure
# Ho = b0 = b1 = b2 = b3 = b4 = b5 = b6 = b7 = 0
# H1 = Atleast one is not equal to zero

# As p value < 0.05 we reject null hypothesis and accept alternate Hypothesis, leading the model is useful as there is a relationship between independent and dependent variables, we can also see that adjusted R^2 is 0.0784.

# We can also observe in relationship between independent and dependent variables individually. We can observe that Hemoglobin independent variable is not satisfying the null hypothesis but all the other variables are satisfying the null hypothesis.
```{r}
reg1 = lm(datatrain$bp~
            datatrain$age+
            datatrain$bu+
            datatrain$pot+
            datatrain$sod+
            datatrain$sc+
            datatrain$hemo+
            datatrain$bgr, data = datatrain)

summary(reg1)

```

# The below plot shows the visual implementation of multiple linear regression model
# The below plot is also used to find the Outliers
```{r}
plot(reg1)
library(car)
outlierTest(reg1)
influenceIndexPlot(reg1)


```

# We are removing outliers 99,149,129 from the dataset for better results
```{r}
datatrain[c(99,149,129),] <- NA
datatrain = na.omit(datatrain)

```

# Again building the new regression model after removing the outliers 

# As p value < 0.05 we reject null hypothesis and accept alternate Hypothesis for the new regression model, leading the model is useful as there is a relationship between independent and dependent variables, we can also see that adjusted R^2 is 0.0767.

# We can also observe in relationship between independent and dependent variables individually. We can observe that Hemoglobin independent variable is not satisfying the null hypothesis but all the other variables are satisfying the null hypothesis.
```{r}
reg2 = lm(datatrain$bp~
            datatrain$age+
            datatrain$bu+
            datatrain$pot+
            datatrain$sod+
            datatrain$sc+
            datatrain$hemo+
            datatrain$bgr, data = datatrain)

summary(reg2)
```

# The coefficients of the variables for new regression model are shown below
```{r}
reg2$coefficients

```

# The residual terms for the new regression model are shown below
```{r}
head(reg2$residuals)
```

# Checking the normality of residuals
# The blow plot shows the residuals are normally distributed
```{r}
qqnorm(reg2$residuals)
qqline(reg2$residuals)
```

# test for normality
# h0 = data is  normally distributed
# h1 = data is not normally distributed
# shapiro test is used to test normality

# As the p value < 0.05 we reject null hypothesis
# Here in shapiro test we have W value as 0.9784 which is very high
```{r}
shapiro.test(reg2$residuals)
```

# The below  plot is used ot check the randomness in the residuals for the new regression model
# The below we are also plotting residual vs fitted values
```{r}
plot(reg2$residuals, c(1:length(reg2$residuals)))
plot(reg2$fitted.values, reg2$residuals)
```
# The below we are checking for Homoscadasticity and heteroscadasticity
##ho = data is homoscadisticity
##h1 = data is hetroscadisticity
## Here we perform bptest for checking the above hypothesis
## As here p-value < 0.05 reject the null hypothesis
```{r}
library(lmtest)
bptest(reg2)

```


## Multi colinearity
# If there is more common varience (common elements) then there will be less proper output
# For doing this multicolinearity we need to install CAR library
# If vif is < 2.5 it is small model saying that there is no multi colinearity problem or no problem
# If vif is < 4 it is small model saying that there is no mmulti colinearity problem or no problem

# As the values are with range there is less/no multi colinearity problem
```{r}
library(car)
vif(reg2)
```

# Checking the normality of error terms
# error = actual - predicted
# For checking normality we do shapiro test, qqnorm, boxplot, histogram
```{r}
boxplot(reg2$residuals)
hist(reg2$residuals)
```

## The below code is used to compare between the the prediction between the trained dataset and the tested dataset, the below values shows the relationship between the numbers as the first number being the trained number and the second number being the test number the amount of nearness gives the good fit for the data

# The below code shows the predicted data of the dependent variable from the regression model

```{r}
predict1 = predict(reg2,datatest)
head(predict1)
```

# The below code shows the actual data of dependent variable
```{r}
actual = data2$bp
head(actual)
```

# The below code shows the residual values of the regression model

```{r}

residuals1 = residuals(reg2)
head(residuals1)

```

# The below code shows the values of Actual, Predicted, Residuals values in a data frame format 

```{r}

results=cbind(predict1,actual, residuals1)
colnames(results)=c('Predicted','Actual','Residuals')
results=as.data.frame(results)
print(head(results))

```


# Calculating the R-squared value by using summary function
```{r}
summary(reg2)$r.squared
```
## AIC and BIC are also penalized-likelihood criteria, as we all know. In a given Bayesian setup, BIC is an approximation of a function of the posterior likelihood of a model being valid, with a lower BIC indicating that a model is more likely to be the true model. This shows the output of the likelyness of the dataset as much low the dataset is the better the dataset
```{r}
AIC(reg2)
BIC(reg2)
```

# we need to use library(metrics) to find RMSE
# Here the RMSE Value is 16.70929 which is low indicates that the model is better fit
```{r}
library(Metrics) 
rmse(predict1,datatest$bp)
```

# As the model reg2 has p -value less than 0.05 making it rejecting the null hypothesis
# We are creating a new model by removing few variables from reg2
# here from we are removing age, blood urea, serum creatinine, hemoglobin as an independent variable and creating a new regression model

# Here we are performing Multiple linear regression
# y = b0 + b1x1 + b2x2 + b3x3

# Null Hypothesis = H0 = b0 = b1 = b2 = b3 = 0
# Alternate Hypothesis = H1 = atleast one is not equal to zero

# As the observed p value > 0.05, we can say that we can accept Null Hypothesis saying that there is no relaitonship between independent variable and dependent variables
# We can also observe the values of Adjusted R-squared as 0.01291 
```{r}
reg3 = lm(datatrain$bp~
            datatrain$pot+
            datatrain$sod+
            datatrain$bgr, data = datatrain)

summary(reg3)
```

# The below plot shows the visual implementation of multiple linear regression model
# The below plot is also used to find the Outliers
# As we have only one Outlier we are continuing with the regression model-3
```{r}
plot(reg3)
library(car)
outlierTest(reg3)
influenceIndexPlot(reg3)
```

# The coefficients of the variables are
```{r}
reg3$coefficients

```

# The residual terms for the new regression model are shown below
```{r}
head(reg3$residuals)
```

# Checking the normality of error terms
# error = actual - predicted
# For checking normality we do shapiro test, qqnorm, boxplot, histogram

# qqnorm, qqline specifies the perfect distribution of values
# In shapiro test as p-value < 0.05 
# Histogram also shows the normality of values
```{r}
qqnorm(reg3$residuals)
qqline(reg3$residuals)
shapiro.test(reg3$residuals)
hist(reg3$residuals)
```

## The below code is used to compare between the the prediction between the trained dataset and the tested dataset, the below values shows the relationship between the numbers as the first number being the trained number and the second number being the test number the amount of nearness gives the good fit for the data
```{r}
predict2 = predict(reg3,datatest)
head(predict2)
```

# The below code shows the actual data of dependent variable
```{r}
actual = data2$bp
head(actual)
```

# The below code shows the residual values of the regression model

```{r}

residuals2 = residuals(reg2)
head(residuals2)

```

```{r}

results1=cbind(predict2,actual, residuals2)
colnames(results1)=c('Predicted','Actual','Residuals')
results1=as.data.frame(results1)
print(head(results1))

```



#we need to use library(metrics) to find RMSE
# Here the RMSE Value is 16.25938 which is low indicates that the model is better fit
```{r}
library(Metrics) 
rmse(predict2,datatest$bp)
```

# Conclusion

## In total after performing linear regression models on dependent variable blood pressure and independent variable age, blood Urea, Potassium, sodium, serum creatinine, hemoglobin, blood glucose random 

## Here by observing the p-value in second regression model reg2 we got our p-value < 0.05 leading us to reject null hypothesis and saying that there is relationship between independent and dependent variable. In this model it gives the normal distribution of residuals. In this we have also observed that there are less outliers present in this model saying that it predicts the blood pressure accurately by using the 7 independent variables(age, blood urea, potassium, sodium, serum creatinine, haemoglobin, and blood glucose random)

## Here by observing the p-value in third regression model reg3 we got our p-value > 0.05 leading us to reject null hypothesis and saying that there is no relationship between independent and dependent variable. the residuals are also less normally distributed. In this even we have less outliers present but the regression model having its p-value > 0.05 saying it is less or no chance in predicting blood pressure using the three independent variables(potassium, sodium, blood glucose random).


## There is a relationship between the variables in the second regression but the model is not a good fit model to implement as it wont satisfy the normality test
## There is no relationship between the variables for third regression model at all.






























